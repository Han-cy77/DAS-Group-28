---
title: "Group_28_Analysis.qmd"
format: html
editor: visual
---

# 1. Background and Aims of the Analysis

With the growing importance of data-informed policymaking, identifying the socioeconomic factors that influence individual income has become a critical area of research. Leveraging the 1994 U.S. Census data, this study seeks to explore how personal and demographic characteristics correlate with income levels, particularly whether an individual earns more than \$50,000 per year.

The dataset includes various attributes such as age, education, marital status, occupation, sex, weekly working hours, and nationality. These features are believed to have different levels of influence on income. To investigate this, the study employs a Generalised Linear Model (GLM) with a logistic link function (logistic regression). In addition to this traditional statistical approach, we also apply machine learning models (e.g., decision trees or random forests) to the same data, allowing us to compare their predictive performance and interpretability against the GLM results.

The research objectives are threefold:

1.  **Feature Identification:** Determine which features derived from census data significantly impact an individual's income level, regardless of specific income thresholds.

2.  **Income Classification:** Identify the key factors that influence whether an individual earns more than \$50,000 per yearâ€”framing the problem as a binary classification task.

3.  **Practical Implications:** Interpret the model results in real-world terms, providing insight into how demographic or behavioral factors (like education level or working hours) may increase the likelihood of higher earnings, and how such findings could inform social and economic policy.

This study also addresses key challenges such as class imbalance, multicollinearity, and categorical data encoding, ensuring the reliability and relevance of the final model outputs. By combining classical statistical modeling with modern machine learning techniques, we aim to produce a robust, interpretable, and practically useful analysis of income determinants.

# 2. Exploratory Data Analysis

## 1. Import data and modify the columns

```{r}
#| message: false
#| echo: false
library(dplyr)
# import dataset and handle missing values
original_data <- read.csv('dataset28.csv', na.strings = '?,')
sum(is.na(original_data))
original_data <- na.omit(original_data)
dim(original_data)

# modify the Education column
# use 'Higher' to represent higher education level
# use 'Lower' to represent lower education level
unique(original_data$Education)
modified_data <- original_data
modified_data <- modified_data %>%
  mutate(
    Education_level = case_when(
      Education %in% c("Bachelors,", "Masters,", "Doctorate,", "Prof-school,") ~ "Higher",
      Education %in% c("Assoc-acdm,", "Assoc-voc,", "Some-college,", "HS-grad,") ~ "Medium",
      TRUE ~ "Lower"
    )
  )

# modify the Marital_Status column
# use 'Yes' or 'No' to represent whether a person has or not has partner
unique(original_data$Marital_Status)
modified_data$Has_partner <- ifelse(original_data$Marital_Status %in% c("Married-civ-spouse,", "Married-AF-spouse,"), "Yes", "No")

# modify the Nationality column
unique(original_data$Nationality)
modified_data$Nationality <- ifelse(original_data$Nationality %in% c('United-States,'), 'US_mainland', 'Others')

# modify the Occupantion column
unique(original_data$Occupation)
modified_data <- modified_data %>%
  mutate(
    Occupation = case_when(
      Occupation %in% c("Exec-managerial,", "Prof-specialty,", "Tech-support,") ~ "High-skilled jobs",
      Occupation %in% c("Adm-clerical,", "Sales,") ~ "Office & Sales jobs",
      Occupation %in% c("Craft-repair,", "Machine-op-inspct,", "Transport-moving,") ~ "Blue-collar jobs",
      TRUE ~ "Service & Labor jobs"
    )
  )

# delete useless columns
modified_data <- modified_data %>%
  dplyr::select(-c(Education, Marital_Status))



# modify other columns
columns_to_clean <- c('Sex', 'Hours_PW')
modified_data[columns_to_clean] <- lapply(modified_data[columns_to_clean], function(x) gsub(",$", "", x))

# check the modified data
dim(modified_data)
summary(modified_data)
str(modified_data)
modified_data$Hours_PW <- as.integer(modified_data$Hours_PW)
modified_data$Age <- as.integer(modified_data$Age)
modified_data$Occupation <- as.factor(modified_data$Occupation)
modified_data$Sex <- as.factor(modified_data$Sex)
modified_data$Nationality <- as.factor(modified_data$Nationality)
modified_data$Income <- as.factor(modified_data$Income)
modified_data$Education_level <- as.factor(modified_data$Education_level)
modified_data$Has_partner <- as.factor(modified_data$Has_partner)
str(modified_data)
```

The dataset, derived from the 1994 U.S. Census, was imported with missing values represented by `"?"`. After removing rows with any missing values (`na.omit()`), the final cleaned dataset contains **30,162 observations** and **8 core variables** relevant to individual socioeconomic characteristics and income level.

**Variable Transformation**

To improve interpretability and reduce dimensionality, several categorical variables were recoded as follows:

-   **Education**: Recoded into a new variable `Education_level` with three ordered categories:

\- \*Lower\* (e.g., primary school)

- \*Medium\* (e.g., high school, some college)

- \*Higher\* (e.g., Bachelors, Masters, Doctorate)

-   **Marital_Status**: Simplified to a binary variable `Has_partner`, indicating whether the individual has a spouse or not.

-   **Nationality**: Consolidated into two categories:

\- \`US_mainland\` for individuals born in the United States

- \`Others\` for all other nationalities

-   **Occupation**: Grouped into four broader categories:

\- \*High-skilled jobs\* (e.g., professionals, tech support, managers)

- \*Office & Sales jobs\*

- \*Blue-collar jobs\* (manual labor, machine operation)

- \*Service & Labor jobs\* (e.g., farming, cleaning, protective services)

Additionally, all character-based variables had trailing commas removed to ensure consistency, and data types were properly cast: numeric variables (`Age`, `Hours_PW`) and categorical variables (`Sex`, `Income`, etc.).

**Descriptive Statistics**

Below is a high-level summary of the main variables after transformation:

-   **Age**: Integer variable, with a wide range from young adults to elderly individuals. Summary statistics will help assess age-related income patterns.

-   **Hours_PW (Hours per Week)**: Integer variable capturing labor intensity. The average weekly working hours is expected to differ significantly between income groups.

-   **Sex**: Categorical (Male/Female). Used to examine gender-based income disparities.

-   **Income**: Binary factor (`<=50K`, `>50K`), our dependent variable for classification modeling.

-   **Education_level**: Factor with ordered levels. Anticipated to show strong correlation with income.

-   **Occupation**: Reclassified into four interpretable job categories, expected to influence earning potential.

-   **Nationality**: Categorical variable used to capture geographic origin and its socioeconomic implications.

-   **Has_partner**: Binary variable which may reflect family structure and economic stability.

**Initial Observations**

-   The dataset is relatively balanced in size and has undergone comprehensive cleaning to remove missing values.

-   Clear categorical consolidation has been applied to reduce dimensionality without sacrificing meaning.

-   The data is now well-structured and ready for GLM modeling and comparison with machine learning methods.

This preprocessing stage sets a solid foundation for robust model estimation and interpretation of income determinants. The modified variables are designed to improve both interpretability and statistical efficiency in subsequent modeling steps.

## 2. Check for Multi-Collinearity issues

```{r}
#| message: false
library(car)
# convert all variables including categorical variables into numeric variables
modified_data_numeric <- modified_data %>%
  # convert Education_level as 1, 2, 3
  mutate(
    Education_level = as.numeric(factor(Education_level, levels = c("Lower", "Medium", "Higher")))
  ) %>%
  # other categorical variables converted to 0 and 1
  mutate(
    Sex = ifelse(Sex == levels(Sex)[1], 0, 1),
    Nationality = ifelse(Nationality == levels(Nationality)[1], 0, 1),
    Income = ifelse(Income == levels(Income)[1], 0, 1),
    Has_partner = ifelse(Has_partner == levels(Has_partner)[1], 0, 1)
  ) %>%
  # others converted to numerical
  mutate_if(is.factor, as.numeric)

VIF_model <- lm(Income ~ ., data = modified_data_numeric)

# calculate VIF
vif_values <- vif(VIF_model)
print(vif_values)

high_vif <- vif_values[vif_values > 10]
if (length(high_vif) > 0) {
  print("Variables with multicollinearity problem:")
  print(high_vif)
} else {
  print("There is no serious multicollinearity problem.")
}
```

## 3. Check sample balance

```{r}
#| message: false
# calculate the sample sizes and proportions of different response
table(modified_data$Income)
prop.table(table(modified_data$Income))
```

## 4. Data visualization

Use chart and plots to summarize the data set.

```{r}
#| message: false
#| label: fig-score1
#| fig-cap: "Relationships Between Income and Ocupation"
#| fig-pos: "H"

library(ggplot2)

# Contingency Tables and Barplots for Categorical variables
# Occupation
Occupation_Income_table <- table(modified_data$Occupation, modified_data$Income)
print(Occupation_Income_table)

ggplot(data = modified_data, mapping = aes(x = Occupation, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Occupation vs Income', x = "Occupation",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

**Occupation vs. Income**

The Figure1 illustrates the relationship between occupation and income distribution:

-    **Blue-collar jobs**: 279 individuals earn **â‰¤50K**, while only 75 earn **\>50K**. This suggests that most blue-collar workers remain in the lower-income bracket.

-    **High-skilled jobs**: 225 individuals earn **â‰¤50K**, while a **significant 187 individuals** earn **\>50K**. This group has the **highest proportion** of high earners.

-    **Office & Sales jobs**: 284 individuals earn **â‰¤50K**, whereas only 54 individuals earn **\>50K**. This suggests that most individuals in this category remain in lower-income brackets.

-    **Service & Labor jobs**: 242 individuals earn **â‰¤50K**, while only 23 exceed the **50K** threshold, indicating **very limited upward income mobility**.

High-skilled jobs show the best income potential, while blue-collar and service jobs predominantly fall within the lower-income group.

```{r}
#| message: false
#| label: fig-score2
#| fig-cap: "Relationships Between Income and Sex"
#| fig-pos: "H"

# Sex
Sex_Income_table <- table(modified_data$Sex, modified_data$Income)
print(Sex_Income_table)

ggplot(data = modified_data, mapping = aes(x = Sex, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Sex vs Income', x = "Sex",
       y = "Count") +
  theme_minimal()
```

**Sex vs. Income**

The Figure2 provides insight into gender-based income disparities:

-   **Females**: 383 earn **â‰¤50K**, while only **51 earn \>50K**. This means only **11.8%** of females in the dataset belong to the high-income group.

-   **Males**: 647 earn **â‰¤50K**, while **288 earn \>50K**. In contrast to females, **30.8%** of males surpass the **50K** income level.

Males are nearly three times more likely to earn \>50K than females (30.8% vs. 11.8%), indicating a substantial gender wage gap. This could be due to occupational segregation, work experience, or systemic wage inequality.

```{r}
#| message: false
#| label: fig-score3
#| fig-cap: "Relationships Between Income and Nationality"
#| fig-pos: "H"

# Nationality
Nationality_Income_table <- table(modified_data$Nationality, modified_data$Income)
print(Nationality_Income_table)

ggplot(data = modified_data, mapping = aes(x = Nationality, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Nationality vs Income', x = "Nationality",
       y = "Count") +
  theme_minimal()
```

**Nationality vs. Income**

The Figure3 explores how income distribution varies by nationality:

-   **US Mainland-born individuals**: 924 earn **â‰¤50K**, while **316 earn \>50K**. This means **25.5%** of this group reaches high-income status.

-   **Others (foreign-born individuals)**: 106 earn **â‰¤50K**, whereas only **23 individuals** earn **\>50K**, meaning only **17.8%** enter the high-income bracket.

Individuals born in the US Mainland are more likely to earn \>50K (25.5%) compared to those born elsewhere (17.8%). This suggests that US-born individuals may have an advantage in accessing higher-paying jobs, possibly due to factors such as language proficiency, education access, or reduced work restrictions.

```{r}
#| message: false
#| label: fig-score4
#| fig-cap: "Relationships Between Income and Education level"
#| fig-pos: "H"

# Education level
Education_Income_table <- table(modified_data$Education_level, modified_data$Income)
print(Education_Income_table)

ggplot(data = modified_data, mapping = aes(x = Education_level, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Education level vs Income', x = "Education level",
       y = "Count") +
  theme_minimal()
```

**Education Level vs. Income**

The Figure4 examines the impact of education on income:

-    **Higher Education (Bachelorâ€™s, Masterâ€™s, Doctorate, etc.)**: **165 earn â‰¤50K**, while **173 earn \>50K**. Notably, this is **the only group where more individuals earn \>50K than â‰¤50K**.

-    **Medium Education (High school graduates, associate degrees, some college)**: **715 earn â‰¤50K**, whereas only **153 earn \>50K**, meaning only **17.6% of this group achieves high income**.

-    **Lower Education (Elementary/Middle school and below)**: **150 earn â‰¤50K**, and only **13 earn \>50K**, meaning an extremely low **8.0%** of this group surpasses the **50K** income level.

Higher education significantly increases the probability of earning \>50K. Nearly 51.2% of highly educated individuals enter the high-income bracket, compared to 17.6% for medium education and just 8.0% for lower education. This highlights education as a major determinant of economic mobility.

```{r}
#| message: false
#| label: fig-score5
#| fig-cap: "Relationships Between Income and Partner"
#| fig-pos: "H"

# Partner
Partner_Income_table <- table(modified_data$Has_partner, modified_data$Income)
print(Partner_Income_table)

ggplot(data = modified_data, mapping = aes(x = Has_partner, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Partner vs Income', x = "Partner",
       y = "Count") +
  theme_minimal()
```

**Marital Status vs. Income (Partner vs. Income)**

The Figure5 investigates the impact of marital status on income:

-    **Individuals without a partner**: **675 earn â‰¤50K**, while only **44 earn \>50K**. This means only **6.1%** of single individuals surpass the **50K** threshold.

-    **Individuals with a partner**: **355 earn â‰¤50K**, while **295 earn \>50K**, meaning **45.4%** earn \>50K.

Key Insight: Individuals with a partner are significantly more likely to earn \>50K (45.4%) than those without a partner (6.1%). This might be due to dual-income households, financial stability, or career advancement associated with family life.

```{r}
#| message: false
#| label: fig-score6
#| fig-cap: "Relationships Between Income and Age"
#| fig-pos: "H"

# Violin plots and boxplots for numerical continuous variables
# Age
ggplot(data = modified_data, aes(x = Income, y = Age, fill = Income)) +
  geom_violin(col = 'transparent') +
  scale_fill_manual(values = c("<=50K" = "#7f7f7f", ">50K" =
                                 "#ffdd57")) +
  labs(title = "Age vs Income", x = "Income", y = "Age") +
  theme_minimal()

ggplot(modified_data, aes(x = Income, y = Age, fill = Income)) +
  geom_boxplot() +
  labs(title = "Age vs Income", x = "Income", y = "Age") +
  theme_minimal()

```

```{r}
#| message: false
#| label: fig-score7
#| fig-cap: "Relationships Between Income and Hours Per Week"
#| fig-pos: "H"

# Hours per week
ggplot(data = modified_data, aes(x = Income, y = Hours_PW, fill = Income)) +
  geom_violin(col = 'transparent') +
  scale_fill_manual(values = c("<=50K" = "#7f7f7f", ">50K" =
                                 "#ffdd57")) +
  labs(title = "Hours Per Week vs Income", x = "Income", y = "Hours_PW") +
  theme_minimal()

ggplot(modified_data, aes(x = Income, y = Hours_PW, fill = Income)) +
  geom_boxplot() +
  labs(title = "Hours Per Week vs Income", x = "Income", y = "Hours per Week") +
  theme_minimal()
```

# 3. Data Splitting: Training set and Test set

```{r}
#| message: false
library(caret)
# set random seed
set.seed(123)

# create training index for stratified sampling
training_index <- createDataPartition(modified_data$Income, p = 0.8, list = FALSE)

# split data into training data and test data
training_data <- modified_data[training_index, ]
test_data  <- modified_data[-training_index, ]
str(training_data)
str(test_data)

# Replace Spaces and ampersand with underscores _
training_data$Occupation <- gsub(" & ", "_", training_data$Occupation)  
training_data$Occupation <- gsub(" ", "_", training_data$Occupation)    

test_data$Occupation <- gsub(" & ", "_", test_data$Occupation)  
test_data$Occupation <- gsub(" ", "_", test_data$Occupation)

# Make sure the variable is still a factor type
training_data$Occupation <- as.factor(training_data$Occupation)

test_data$Occupation <- as.factor(test_data$Occupation)

# View the modified categories
levels(training_data$Occupation)

levels(test_data$Occupation)




# check dimension
dim(training_data)
dim(test_data)

# check distribution in both datasets
prop.table(table(training_data$Income))
prop.table(table(test_data$Income))
```

# 4. Statistical Modelling and Results

## 4.1 GLM

### 4.1.1 GLM Model fitting

```{r}
#| message: false
library(MASS)
library(caret)
library(pROC)
library(pscl)

# Create dummy variables
dummy_variables <- model.matrix(~ Occupation - 1, data = training_data)
education_dummy <- model.matrix(~ Education_level - 1, data = training_data)
nationality_dummy <- model.matrix(~ Nationality - 1, data = training_data)
has_partner_dummy <- model.matrix(~ Has_partner - 1, data = training_data)
sex_dummy <- model.matrix(~ Sex - 1, data = training_data)
has_partner_dummy <- model.matrix(~ Has_partner - 1, data = training_data)

#Handle non-standard characters
col_names <- c("OccupationBlue-collar jobs",    
               "OccupationHigh-skilled jobs",  
               "OccupationOffice & Sales jobs",  
               "OccupationService & Labor jobs")
clean_col_names <- gsub("[^a-zA-Z0-9]", "_", col_names)   
clean_col_names <- gsub("_+", "_", clean_col_names) 
clean_col_names <- gsub("_$", "", clean_col_names) 
print(clean_col_names)

colnames(training_data_GLM)[which(colnames(training_data_GLM) %in% col_names)] <- clean_col_names


#Check new variables
colnames(dummy_variables)
colnames(education_dummy)
colnames(nationality_dummy)
colnames(has_partner_dummy)
colnames(sex_dummy)

# Define training_data_GLM
training_data_GLM <- training_data
# Add the dummy variables into the original dataset
training_data_GLM <- cbind(training_data_GLM, dummy_variables)
training_data_GLM  <- cbind(training_data_GLM, education_dummy)
training_data_GLM  <- cbind(training_data_GLM, nationality_dummy)
training_data_GLM  <- cbind(training_data_GLM, has_partner_dummy)
training_data_GLM  <- cbind(training_data_GLM, sex_dummy)

full_model <- glm(Income ~ (Age + Occupation + Sex + Hours_PW + Nationality + Education_level + Has_partner)^2, 
                  data = training_data_GLM , family = binomial)


step_model <- stepAIC(full_model, direction = "both", trace = FALSE)
summary(step_model)


test_data$predicted_prob <- predict(step_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob >= 0.5, ">50K", "<=50K")
test_data$predicted_class <- factor(test_data$predicted_class, levels = levels(test_data$Income))


conf_matrix <- confusionMatrix(test_data$predicted_class, test_data$Income)
print(conf_matrix)


roc_curve <- roc(test_data$Income, test_data$predicted_prob)
auc_score <- auc(roc_curve)
print(auc_score)
plot(roc_curve, col = "blue", main = "ROC Curve for Logistic Regression")


pseudo_r2 <- pR2(step_model)
print(pseudo_r2)

```

### 4.1.2 GLM Model optimization

```{r}
# Load necessary libraries
library(MASS)  # stepAIC()
library(car)   # VIF
library(pROC)  # AUC calculation
library(dplyr) # Data manipulation

# Step 1: Select the best main effects model
base_model <- glm(Income ~ Age + Occupation + Sex + Hours_PW + Nationality + Education_level + Has_partner, 
                  data = training_data_GLM , family = binomial)

step_base_model <- stepAIC(base_model, direction = "both", trace = FALSE)

summary(step_base_model)  # View the optimized main effects model

# Step 2: Check interaction effects
interaction_model <- glm(Income ~ Age + Occupation + Hours_PW + Nationality + 
                           Education_level + Has_partner +
                           Education_level:Occupation + Hours_PW:Occupation,
                         data = training_data_GLM , family = binomial)

anova(step_base_model, interaction_model, test = "Chisq")  # Perform likelihood ratio test

# Step 3: Use stepAIC() to select the final model
final_model <- stepAIC(interaction_model, direction = "both", trace = FALSE)

summary(final_model)  # View the final model

# Step 4: Generate dummy variables (Ensure that OccupationService_Labor_jobs exists)
training_data$Occupation <- as.factor(training_data$Occupation)
occupation_dummy <- model.matrix(~ Occupation - 1, data = training_data_GLM ) %>% as.data.frame()
training_data <- cbind(training_data_GLM , occupation_dummy)

# Check if `OccupationService_Labor_jobs` is included
colnames(training_data)

View(training_data_GLM)




# Step 5: Refit the final optimized model
final_model_refined <- glm(Income ~ Age + OccupationService_Labor_jobs + Hours_PW + 
                             Nationality + Education_level + Has_partner + 
                             OccupationService_Labor_jobs:Hours_PW, 
                           family = binomial, data = training_data_GLM)

summary(final_model_refined)  # Check statistical significance

# Step 6: Check VIF (Multicollinearity)
vif(final_model_refined)

# Standardize variables
training_data_GLM $Hours_PW <- scale(training_data_GLM $Hours_PW)
training_data_GLM $OccupationService_Labor_jobs <- scale(training_data_GLM $OccupationService_Labor_jobs)

# Refit the model
final_model_refined <- glm(Income ~ Age + OccupationService_Labor_jobs + Hours_PW + 
                             Nationality + Education_level + Has_partner + 
                             OccupationService_Labor_jobs:Hours_PW, 
                           family = binomial, data = training_data_GLM )

# Recheck VIF
vif(final_model_refined)

# Step 7: Compute AUC to evaluate classification performance
roc_curve <- roc(training_data_GLM $Income, fitted(final_model_refined))
auc(roc_curve)  # Compute AUC value
plot(roc_curve) # Plot ROC curve

```

In the model optimization process, we first selected the optimal main effects model by applying main effect selection, ANOVA interaction testing, and stepwise AIC optimization, aiming to construct a more robust model. Variables with statistical significance (p \< 0.05) were retained, while non-significant variables (p \> 0.1) were removed. The residual deviance of the current model was 773.01, indicating that it explained a substantial amount of the data variability. The AIC value was 793.01, and the Number of Fisher Scoring iterations was 6, suggesting that the model was relatively stable.

Subsequently, we examined potential interaction effects to assess their additional explanatory power. The ANOVA test revealed significant interaction effects (p \< 0.05), leading to further optimization using stepAIC(), which resulted in the final model. To evaluate the modelâ€™s predictive performance, we computed the AUC, obtaining a value of 0.8871, which suggests a strong predictive ability.

We then examined multicollinearity using the VIF test. The results initially indicated that OccupationService_Labor_jobs (VIF = 12.626) and OccupationService_Labor_jobs:Hours_PW (VIF = 12.196) had VIF values exceeding 10, suggesting a severe multicollinearity issue. However, since OccupationService_Labor_jobs:Hours_PW was statistically significant (p \< 0.05), it was not appropriate to simply remove this interaction term. Instead, we applied variable standardization to mitigate collinearity. After standardizing the relevant variables, a subsequent VIF test confirmed that all VIF values were below 2, indicating that the collinearity issue was effectively resolved.

## 4.2 Random Forests

```{r}
library(randomForest)  
library(caret)        
library(pROC)         
library(rpart.plot)

training_data <- modified_data[training_index, ]
test_data  <- modified_data[-training_index, ]

training_data$Income <- as.factor(training_data$Income)
test_data$Income <- as.factor(test_data$Income)

# Ensure that the target variable is a factor type
training_data$Income <- as.factor(training_data$Income)
test_data$Income <- as.factor(test_data$Income)


# Constructing random forest model
set.seed(123)  
rf_model <- randomForest(
  Income ~ .,                
  data = training_data,      
  ntree = 500,               
  mtry = sqrt(ncol(training_data) - 1), importance = TRUE, proximity = TRUE)  

# View model summary
print(rf_model)
plot(rf_model) 
# Extract the first tree in a random forest
tree <- getTree(rf_model, k = 1, labelVar = TRUE)
print(tree)

# Extract OOB errors
oob_errors <- rf_model$err.rate

# Convert OOB errors to a data frame for plotting
oob_df <- data.frame(
  Trees = 1:nrow(oob_errors),
  OOB_Error = oob_errors[, "OOB"])

# Plot the OOB error curve
ggplot(oob_df, aes(x = Trees, y = OOB_Error)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "OOB Error Curve for Random Forest",
    x = "Number of Trees",
    y = "OOB Error") 



# Variable importance analysis
importance(rf_model)
varImpPlot(rf_model, main = "Variable Importance Plot")

#Define test_data_rf
test_data_rf <- test_data
# Prediction on test set
test_data_rf$predicted_prob <- predict(rf_model, newdata = test_data, type = "prob")[, 2]
test_data_rf$predicted_class <- predict(rf_model, newdata = test_data, type = "response")

# Calculate confusion matrix
conf_matrix <- confusionMatrix(test_data_rf$predicted_class, test_data_rf$Income)
print(conf_matrix)

# Calculate ROC-AUC
roc_curve <- roc(test_data_rf$Income, test_data_rf$predicted_prob)
auc_score <- auc(roc_curve)
print(auc_score)
plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")


```

The OOB error rate is 17.06%, which means the model misclassifies approximately 17.06% of the training data on average. This is a good estimate of the model's generalization error. The model achieves good accuracy (80.59%) and a high AUC score (0.8632), indicating strong overall performance. However, the lower specificity for the \>50K class highlights the need to address class imbalance and refine the model further. Has_partner is the most important variable for accuracy, suggesting it plays a critical role in predicting income. From the variable importance plot, we can indicate that Age and Education_level are also highly important, which aligns with real-world intuition (income often correlates with age and education).

# 5. Conclusions and Future work
