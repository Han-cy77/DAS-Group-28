---
title: "Group_28_Analysis.qmd"
format: html
editor: visual
---

# 1. Background and Target of Research

With the growing importance of data-informed policymaking, identifying the socioeconomic factors that influence individual income has become a critical area of research. Leveraging the 1994 U.S. Census data, this study seeks to explore how personal and demographic characteristics correlate with income levels, particularly whether an individual earns more than \$50,000 per year.

The dataset includes various attributes such as age, education, marital status, occupation, sex, weekly working hours, and nationality. These features are believed to have different levels of influence on income. To investigate this, the study employs a Generalised Linear Model (GLM) with a logistic link function (logistic regression). In addition to this traditional statistical approach, we also apply machine learning models (e.g., decision trees or random forests) to the same data, allowing us to compare their predictive performance and interpretability against the GLM results.

The research objectives are threefold:

1.  **Feature Identification:** Determine which features derived from census data significantly impact an individual's income level, regardless of specific income thresholds.

2.  **Income Classification:** Identify the key factors that influence whether an individual earns more than \$50,000 per year—framing the problem as a binary classification task.

3.  **Practical Implications:** Interpret the model results in real-world terms, providing insight into how demographic or behavioral factors (like education level or working hours) may increase the likelihood of higher earnings, and how such findings could inform social and economic policy.

This study also addresses key challenges such as class imbalance, multicollinearity, and categorical data encoding, ensuring the reliability and relevance of the final model outputs. By combining classical statistical modeling with modern machine learning techniques, we aim to produce a robust, interpretable, and practically useful analysis of income determinants.

# 2. Exploratory Data analysis

## 1. Import data and modify the columns

```{r}
#| message: false
library(dplyr)
# import dataset and handle missing values
original_data <- read.csv('dataset28.csv', na.strings = '?,')
sum(is.na(original_data))
original_data <- na.omit(original_data)
dim(original_data)

# modify the Education column
# use 'Higher' to represent higher education level
# use 'Lower' to represent lower education level
unique(original_data$Education)
modified_data <- original_data
modified_data <- modified_data %>%
  mutate(
    Education_level = case_when(
      Education %in% c("Bachelors,", "Masters,", "Doctorate,", "Prof-school,") ~ "Higher",
      Education %in% c("Assoc-acdm,", "Assoc-voc,", "Some-college,", "HS-grad,") ~ "Medium",
      TRUE ~ "Lower"
    )
  )

# modify the Marital_Status column
# use 'Yes' or 'No' to represent whether a person has or not has partner
unique(original_data$Marital_Status)
modified_data$Has_partner <- ifelse(original_data$Marital_Status %in% c("Married-civ-spouse,", "Married-AF-spouse,"), "Yes", "No")

# modify the Nationality column
unique(original_data$Nationality)
modified_data$Nationality <- ifelse(original_data$Nationality %in% c('United-States,'), 'US_mainland', 'Others')

# modify the Occupantion column
unique(original_data$Occupation)
modified_data <- modified_data %>%
  mutate(
    Occupation = case_when(
      Occupation %in% c("Exec-managerial,", "Prof-specialty,", "Tech-support,") ~ "High-skilled jobs",
      Occupation %in% c("Adm-clerical,", "Sales,") ~ "Office & Sales jobs",
      Occupation %in% c("Craft-repair,", "Machine-op-inspct,", "Transport-moving,") ~ "Blue-collar jobs",
      TRUE ~ "Service & Labor jobs"
    )
  )

# delete useless columns
modified_data <- modified_data %>%
  dplyr::select(-c(Education, Marital_Status))



# modify other columns
columns_to_clean <- c('Sex', 'Hours_PW')
modified_data[columns_to_clean] <- lapply(modified_data[columns_to_clean], function(x) gsub(",$", "", x))

# check the modified data
dim(modified_data)
summary(modified_data)
str(modified_data)
modified_data$Hours_PW <- as.integer(modified_data$Hours_PW)
modified_data$Age <- as.integer(modified_data$Age)
modified_data$Occupation <- as.factor(modified_data$Occupation)
modified_data$Sex <- as.factor(modified_data$Sex)
modified_data$Nationality <- as.factor(modified_data$Nationality)
modified_data$Income <- as.factor(modified_data$Income)
modified_data$Education_level <- as.factor(modified_data$Education_level)
modified_data$Has_partner <- as.factor(modified_data$Has_partner)
str(modified_data)
```

The dataset, derived from the 1994 U.S. Census, was imported with missing values represented by `"?"`. After removing rows with any missing values (`na.omit()`), the final cleaned dataset contains **30,162 observations** and **8 core variables** relevant to individual socioeconomic characteristics and income level.

#### **Variable Transformation**

To improve interpretability and reduce dimensionality, several categorical variables were recoded as follows:

-   **Education**: Recoded into a new variable `Education_level` with three ordered categories:

```         
-   *Lower* (e.g., primary school)

-    *Medium* (e.g., high school, some college)

-    *Higher* (e.g., Bachelors, Masters, Doctorate)
```

-   **Marital_Status**: Simplified to a binary variable `Has_partner`, indicating whether the individual has a spouse or not.

-   **Nationality**: Consolidated into two categories:

```         
-    `US_mainland` for individuals born in the United States

-    `Others` for all other nationalities
```

-   **Occupation**: Grouped into four broader categories:

```         
-    *High-skilled jobs* (e.g., professionals, tech support, managers)

-    *Office & Sales jobs*

-    *Blue-collar jobs* (manual labor, machine operation)

-    *Service & Labor jobs* (e.g., farming, cleaning, protective services)
```

Additionally, all character-based variables had trailing commas removed to ensure consistency, and data types were properly cast: numeric variables (`Age`, `Hours_PW`) and categorical variables (`Sex`, `Income`, etc.).

#### **Descriptive Statistics**

Below is a high-level summary of the main variables after transformation:

-   **Age**: Integer variable, with a wide range from young adults to elderly individuals. Summary statistics will help assess age-related income patterns.

-   **Hours_PW (Hours per Week)**: Integer variable capturing labor intensity. The average weekly working hours is expected to differ significantly between income groups.

-   **Sex**: Categorical (Male/Female). Used to examine gender-based income disparities.

-   **Income**: Binary factor (`<=50K`, `>50K`), our dependent variable for classification modeling.

-   **Education_level**: Factor with ordered levels. Anticipated to show strong correlation with income.

-   **Occupation**: Reclassified into four interpretable job categories, expected to influence earning potential.

-   **Nationality**: Categorical variable used to capture geographic origin and its socioeconomic implications.

-   **Has_partner**: Binary variable which may reflect family structure and economic stability.

#### **Initial Observations**

-   The dataset is relatively balanced in size and has undergone comprehensive cleaning to remove missing values.

-   Clear categorical consolidation has been applied to reduce dimensionality without sacrificing meaning.

-   The data is now well-structured and ready for GLM modeling and comparison with machine learning methods.

This preprocessing stage sets a solid foundation for robust model estimation and interpretation of income determinants. The modified variables are designed to improve both interpretability and statistical efficiency in subsequent modeling steps.

## 2. Check for Multi-Collinearity issues

```{r}
#| message: false
library(car)
# convert all variables including categorical variables into numeric variables
modified_data_numeric <- modified_data %>%
  # convert Education_level as 1, 2, 3
  mutate(
    Education_level = as.numeric(factor(Education_level, levels = c("Lower", "Medium", "Higher")))
  ) %>%
  # other categorical variables converted to 0 and 1
  mutate(
    Sex = ifelse(Sex == levels(Sex)[1], 0, 1),
    Nationality = ifelse(Nationality == levels(Nationality)[1], 0, 1),
    Income = ifelse(Income == levels(Income)[1], 0, 1),
    Has_partner = ifelse(Has_partner == levels(Has_partner)[1], 0, 1)
  ) %>%
  # others converted to numerical
  mutate_if(is.factor, as.numeric)

VIF_model <- lm(Income ~ ., data = modified_data_numeric)

# calculate VIF
vif_values <- vif(VIF_model)
print(vif_values)

high_vif <- vif_values[vif_values > 10]
if (length(high_vif) > 0) {
  print("Variables with multicollinearity problem:")
  print(high_vif)
} else {
  print("There is no serious multicollinearity problem.")
}
```

## 3. Check sample balance

```{r}
#| message: false
# calculate the sample sizes and proportions of different response
table(modified_data$Income)
prop.table(table(modified_data$Income))
```

## 4. Data visualization

Use chart and plots to summarize the data set.

```{r}
#| message: false
library(ggplot2)

# Contingency Tables and Barplots for Categorical variables
# Occupation
Occupation_Income_table <- table(modified_data$Occupation, modified_data$Income)
print(Occupation_Income_table)

ggplot(data = modified_data, mapping = aes(x = Occupation, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Occupation vs Income', x = "Occupation",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

```{r}
#| message: false
# Sex
Sex_Income_table <- table(modified_data$Sex, modified_data$Income)
print(Sex_Income_table)

ggplot(data = modified_data, mapping = aes(x = Sex, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Sex vs Income', x = "Sex",
       y = "Count") +
  theme_minimal()
```

```{r}
#| message: false
# Nationality
Nationality_Income_table <- table(modified_data$Nationality, modified_data$Income)
print(Nationality_Income_table)

ggplot(data = modified_data, mapping = aes(x = Nationality, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Nationality vs Income', x = "Nationality",
       y = "Count") +
  theme_minimal()
```

```{r}
#| message: false
# Education level
Education_Income_table <- table(modified_data$Education_level, modified_data$Income)
print(Education_Income_table)

ggplot(data = modified_data, mapping = aes(x = Education_level, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Education level vs Income', x = "Education level",
       y = "Count") +
  theme_minimal()
```

```{r}
#| message: false
# Partner
Partner_Income_table <- table(modified_data$Has_partner, modified_data$Income)
print(Partner_Income_table)

ggplot(data = modified_data, mapping = aes(x = Has_partner, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Partner vs Income', x = "Partner",
       y = "Count") +
  theme_minimal()
```

```{r}
#| message: false
# Violin plots and boxplots for numerical continuous variables
# Age
ggplot(data = modified_data, aes(x = Income, y = Age, fill = Income)) +
  geom_violin(col = 'transparent') +
  scale_fill_manual(values = c("<=50K" = "#7f7f7f", ">50K" =
                                 "#ffdd57")) +
  labs(title = "Age vs Income", x = "Income", y = "Age") +
  theme_minimal()

ggplot(modified_data, aes(x = Income, y = Age, fill = Income)) +
  geom_boxplot() +
  labs(title = "Age vs Income", x = "Age", y = "Hours per Week") +
  theme_minimal()

# Hours per week
ggplot(data = modified_data, aes(x = Income, y = Hours_PW, fill = Income)) +
  geom_violin(col = 'transparent') +
  scale_fill_manual(values = c("<=50K" = "#7f7f7f", ">50K" =
                                 "#ffdd57")) +
  labs(title = "Hours Per Week vs Income", x = "Income", y = "Hours_PW") +
  theme_minimal()

ggplot(modified_data, aes(x = Income, y = Hours_PW, fill = Income)) +
  geom_boxplot() +
  labs(title = "Hours Per Week vs Income", x = "Income", y = "Hours per Week") +
  theme_minimal()
```

# 3. Data Splitting: Training set and Test set

```{r}
#| message: false
library(caret)
# set random seed
set.seed(123)

# create training index for stratified sampling
training_index <- createDataPartition(modified_data$Income, p = 0.8, list = FALSE)

# split data into training data and test data
training_data <- modified_data[training_index, ]
test_data  <- modified_data[-training_index, ]
str(training_data)
str(test_data)

# Replace Spaces and ampersand with underscores _
training_data$Occupation <- gsub(" & ", "_", training_data$Occupation)  
training_data$Occupation <- gsub(" ", "_", training_data$Occupation)    

test_data$Occupation <- gsub(" & ", "_", test_data$Occupation)  
test_data$Occupation <- gsub(" ", "_", test_data$Occupation)

# Make sure the variable is still a factor type
training_data$Occupation <- as.factor(training_data$Occupation)

test_data$Occupation <- as.factor(test_data$Occupation)

# View the modified categories
levels(training_data$Occupation)

levels(test_data$Occupation)




# check dimension
dim(training_data)
dim(test_data)

# check distribution in both datasets
prop.table(table(training_data$Income))
prop.table(table(test_data$Income))
```

# 4. Model fitting

## 4.1 GLM

### 4.1.1 GLM Model fitting

```{r}
#| message: false
library(MASS)
library(caret)
library(pROC)
library(pscl)

# Create dummy variables
dummy_variables <- model.matrix(~ Occupation - 1, data = training_data)
education_dummy <- model.matrix(~ Education_level - 1, data = training_data)
nationality_dummy <- model.matrix(~ Nationality - 1, data = training_data)
has_partner_dummy <- model.matrix(~ Has_partner - 1, data = training_data)
sex_dummy <- model.matrix(~ Sex - 1, data = training_data)
has_partner_dummy <- model.matrix(~ Has_partner - 1, data = training_data)

#Handle non-standard characters
col_names <- c("OccupationBlue-collar jobs",    
               "OccupationHigh-skilled jobs",  
               "OccupationOffice & Sales jobs",  
               "OccupationService & Labor jobs")
clean_col_names <- gsub("[^a-zA-Z0-9]", "_", col_names)   
clean_col_names <- gsub("_+", "_", clean_col_names) 
clean_col_names <- gsub("_$", "", clean_col_names) 
print(clean_col_names)

colnames(training_data_GLM)[which(colnames(training_data_GLM) %in% col_names)] <- clean_col_names


#Check new variables
colnames(dummy_variables)
colnames(education_dummy)
colnames(nationality_dummy)
colnames(has_partner_dummy)
colnames(sex_dummy)

# Define training_data_GLM
training_data_GLM <- training_data
# Add the dummy variables into the original dataset
training_data_GLM <- cbind(training_data_GLM, dummy_variables)
training_data_GLM  <- cbind(training_data_GLM, education_dummy)
training_data_GLM  <- cbind(training_data_GLM, nationality_dummy)
training_data_GLM  <- cbind(training_data_GLM, has_partner_dummy)
training_data_GLM  <- cbind(training_data_GLM, sex_dummy)

full_model <- glm(Income ~ (Age + Occupation + Sex + Hours_PW + Nationality + Education_level + Has_partner)^2, 
                  data = training_data_GLM , family = binomial)


step_model <- stepAIC(full_model, direction = "both", trace = FALSE)
summary(step_model)


test_data$predicted_prob <- predict(step_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob >= 0.5, ">50K", "<=50K")
test_data$predicted_class <- factor(test_data$predicted_class, levels = levels(test_data$Income))


conf_matrix <- confusionMatrix(test_data$predicted_class, test_data$Income)
print(conf_matrix)


roc_curve <- roc(test_data$Income, test_data$predicted_prob)
auc_score <- auc(roc_curve)
print(auc_score)
plot(roc_curve, col = "blue", main = "ROC Curve for Logistic Regression")


pseudo_r2 <- pR2(step_model)
print(pseudo_r2)

```

### 4.1.2 GLM Model optimization

```{r}
# Load necessary libraries
library(MASS)  # stepAIC()
library(car)   # VIF
library(pROC)  # AUC calculation
library(dplyr) # Data manipulation

# Step 1: Select the best main effects model
base_model <- glm(Income ~ Age + Occupation + Sex + Hours_PW + Nationality + Education_level + Has_partner, 
                  data = training_data_GLM , family = binomial)

step_base_model <- stepAIC(base_model, direction = "both", trace = FALSE)

summary(step_base_model)  # View the optimized main effects model

# Step 2: Check interaction effects
interaction_model <- glm(Income ~ Age + Occupation + Hours_PW + Nationality + 
                           Education_level + Has_partner +
                           Education_level:Occupation + Hours_PW:Occupation,
                         data = training_data_GLM , family = binomial)

anova(step_base_model, interaction_model, test = "Chisq")  # Perform likelihood ratio test

# Step 3: Use stepAIC() to select the final model
final_model <- stepAIC(interaction_model, direction = "both", trace = FALSE)

summary(final_model)  # View the final model

# Step 4: Generate dummy variables (Ensure that OccupationService_Labor_jobs exists)
training_data$Occupation <- as.factor(training_data$Occupation)
occupation_dummy <- model.matrix(~ Occupation - 1, data = training_data_GLM ) %>% as.data.frame()
training_data <- cbind(training_data_GLM , occupation_dummy)

# Check if `OccupationService_Labor_jobs` is included
colnames(training_data)

View(training_data_GLM)




# Step 5: Refit the final optimized model
final_model_refined <- glm(Income ~ Age + OccupationService_Labor_jobs + Hours_PW + 
                             Nationality + Education_level + Has_partner + 
                             OccupationService_Labor_jobs:Hours_PW, 
                           family = binomial, data = training_data_GLM)

summary(final_model_refined)  # Check statistical significance

# Step 6: Check VIF (Multicollinearity)
vif(final_model_refined)

# Standardize variables
training_data_GLM $Hours_PW <- scale(training_data_GLM $Hours_PW)
training_data_GLM $OccupationService_Labor_jobs <- scale(training_data_GLM $OccupationService_Labor_jobs)

# Refit the model
final_model_refined <- glm(Income ~ Age + OccupationService_Labor_jobs + Hours_PW + 
                             Nationality + Education_level + Has_partner + 
                             OccupationService_Labor_jobs:Hours_PW, 
                           family = binomial, data = training_data_GLM )

# Recheck VIF
vif(final_model_refined)

# Step 7: Compute AUC to evaluate classification performance
roc_curve <- roc(training_data_GLM $Income, fitted(final_model_refined))
auc(roc_curve)  # Compute AUC value
plot(roc_curve) # Plot ROC curve

```

In the model optimization process, we first selected the optimal main effects model by applying main effect selection, ANOVA interaction testing, and stepwise AIC optimization, aiming to construct a more robust model. Variables with statistical significance (p \< 0.05) were retained, while non-significant variables (p \> 0.1) were removed. The residual deviance of the current model was 773.01, indicating that it explained a substantial amount of the data variability. The AIC value was 793.01, and the Number of Fisher Scoring iterations was 6, suggesting that the model was relatively stable.

Subsequently, we examined potential interaction effects to assess their additional explanatory power. The ANOVA test revealed significant interaction effects (p \< 0.05), leading to further optimization using stepAIC(), which resulted in the final model. To evaluate the model’s predictive performance, we computed the AUC, obtaining a value of 0.8871, which suggests a strong predictive ability.

We then examined multicollinearity using the VIF test. The results initially indicated that OccupationService_Labor_jobs (VIF = 12.626) and OccupationService_Labor_jobs:Hours_PW (VIF = 12.196) had VIF values exceeding 10, suggesting a severe multicollinearity issue. However, since OccupationService_Labor_jobs:Hours_PW was statistically significant (p \< 0.05), it was not appropriate to simply remove this interaction term. Instead, we applied variable standardization to mitigate collinearity. After standardizing the relevant variables, a subsequent VIF test confirmed that all VIF values were below 2, indicating that the collinearity issue was effectively resolved.

## 4.2 Random Forests

```{r}
library(randomForest)  
library(caret)        
library(pROC)         
library(rpart.plot)

training_data <- modified_data[training_index, ]
test_data  <- modified_data[-training_index, ]

training_data$Income <- as.factor(training_data$Income)
test_data$Income <- as.factor(test_data$Income)

# Ensure that the target variable is a factor type
training_data$Income <- as.factor(training_data$Income)
test_data$Income <- as.factor(test_data$Income)


# Constructing random forest model
set.seed(123)  
rf_model <- randomForest(
  Income ~ .,                
  data = training_data,      
  ntree = 500,               
  mtry = sqrt(ncol(training_data) - 1), importance = TRUE, proximity = TRUE)  

# View model summary
print(rf_model)
plot(rf_model) 
# Extract the first tree in a random forest
tree <- getTree(rf_model, k = 1, labelVar = TRUE)
print(tree)

# Extract OOB errors
oob_errors <- rf_model$err.rate

# Convert OOB errors to a data frame for plotting
oob_df <- data.frame(
  Trees = 1:nrow(oob_errors),
  OOB_Error = oob_errors[, "OOB"])

# Plot the OOB error curve
ggplot(oob_df, aes(x = Trees, y = OOB_Error)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "OOB Error Curve for Random Forest",
    x = "Number of Trees",
    y = "OOB Error") 



# Variable importance analysis
importance(rf_model)
varImpPlot(rf_model, main = "Variable Importance Plot")

#Define test_data_rf
test_data_rf <- test_data
# Prediction on test set
test_data_rf$predicted_prob <- predict(rf_model, newdata = test_data, type = "prob")[, 2]
test_data_rf$predicted_class <- predict(rf_model, newdata = test_data, type = "response")

# Calculate confusion matrix
conf_matrix <- confusionMatrix(test_data_rf$predicted_class, test_data_rf$Income)
print(conf_matrix)

# Calculate ROC-AUC
roc_curve <- roc(test_data_rf$Income, test_data_rf$predicted_prob)
auc_score <- auc(roc_curve)
print(auc_score)
plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")


```

The OOB error rate is 17.06%, which means the model misclassifies approximately 17.06% of the training data on average. This is a good estimate of the model's generalization error. The model achieves good accuracy (80.59%) and a high AUC score (0.8632), indicating strong overall performance. However, the lower specificity for the \>50K class highlights the need to address class imbalance and refine the model further. Has_partner is the most important variable for accuracy, suggesting it plays a critical role in predicting income. From the variable importance plot, we can indicate that Age and Education_level are also highly important, which aligns with real-world intuition (income often correlates with age and education).
