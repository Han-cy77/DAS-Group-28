---
title: "Group_28_Analysis.qmd"
format: html
editor: visual
---

# 1. Background and Target of Research

# 2. Exploratory Data analysis

## 1. Import data and modify the columns

```{r}
#| message: false
library(dplyr)
# import dataset and handle missing values
original_data <- read.csv('dataset28.csv', na.strings = '?,')
sum(is.na(original_data))
original_data <- na.omit(original_data)
dim(original_data)

# modify the Education column
# use 'Higher' to represent higher education level
# use 'Lower' to represent lower education level
unique(original_data$Education)
modified_data <- original_data
modified_data <- modified_data %>%
  mutate(
    Education_level = case_when(
      Education %in% c("Bachelors,", "Masters,", "Doctorate,", "Prof-school,") ~ "Higher",
      Education %in% c("Assoc-acdm,", "Assoc-voc,", "Some-college,", "HS-grad,") ~ "Medium",
      TRUE ~ "Lower"
    )
  )

# modify the Marital_Status column
# use 'Yes' or 'No' to represent whether a person has or not has partner
unique(original_data$Marital_Status)
modified_data$Has_partner <- ifelse(original_data$Marital_Status %in% c("Married-civ-spouse,", "Married-AF-spouse,"), "Yes", "No")

# modify the Nationality column
unique(original_data$Nationality)
modified_data$Nationality <- ifelse(original_data$Nationality %in% c('United-States,'), 'US_mainland', 'Others')

# modify the Occupantion column
unique(original_data$Occupation)
modified_data <- modified_data %>%
  mutate(
    Occupation = case_when(
      Occupation %in% c("Exec-managerial,", "Prof-specialty,", "Tech-support,") ~ "High-skilled jobs",
      Occupation %in% c("Adm-clerical,", "Sales,") ~ "Office & Sales jobs",
      Occupation %in% c("Craft-repair,", "Machine-op-inspct,", "Transport-moving,") ~ "Blue-collar jobs",
      TRUE ~ "Service & Labor jobs"
    )
  )

# delete useless columns
modified_data <- modified_data %>%
  dplyr::select(-Education, -Marital_Status)


# modify other columns
columns_to_clean <- c('Sex', 'Hours_PW')
modified_data[columns_to_clean] <- lapply(modified_data[columns_to_clean], function(x) gsub(",$", "", x))

# check the modified data
dim(modified_data)
summary(modified_data)
str(modified_data)
modified_data$Hours_PW <- as.integer(modified_data$Hours_PW)
modified_data$Age <- as.integer(modified_data$Age)
modified_data$Occupation <- as.factor(modified_data$Occupation)
modified_data$Sex <- as.factor(modified_data$Sex)
modified_data$Nationality <- as.factor(modified_data$Nationality)
modified_data$Income <- as.factor(modified_data$Income)
modified_data$Education_level <- as.factor(modified_data$Education_level)
modified_data$Has_partner <- as.factor(modified_data$Has_partner)
str(modified_data)
```

## 2. Check for Multi-Collinearity issues

```{r}
#| message: false
library(car)
# convert all variables including categorical variables into numeric variables
modified_data_numeric <- modified_data %>%
  # convert Education_level as 1, 2, 3
  mutate(
    Education_level = as.numeric(factor(Education_level, levels = c("Lower", "Medium", "Higher")))
  ) %>%
  # other categorical variables converted to 0 and 1
  mutate(
    Sex = ifelse(Sex == levels(Sex)[1], 0, 1),
    Nationality = ifelse(Nationality == levels(Nationality)[1], 0, 1),
    Income = ifelse(Income == levels(Income)[1], 0, 1),
    Has_partner = ifelse(Has_partner == levels(Has_partner)[1], 0, 1)
  ) %>%
  # others converted to numerical
  mutate_if(is.factor, as.numeric)

VIF_model <- lm(Income ~ ., data = modified_data_numeric)

# calculate VIF
vif_values <- vif(VIF_model)
print(vif_values)

high_vif <- vif_values[vif_values > 10]
if (length(high_vif) > 0) {
  print("Variables with multicollinearity problem:")
  print(high_vif)
} else {
  print("There is no serious multicollinearity problem.")
}
```

## 3. Check sample balance

```{r}
#| message: false
# calculate the sample sizes and proportions of different response
table(modified_data$Income)
prop.table(table(modified_data$Income))
```

## 4. Data visualization

Use chart and plots to summarize the data set.

```{r}
#| message: false
library(ggplot2)

# Contingency Tables and Barplots for Categorical variables
# Occupation
Occupation_Income_table <- table(modified_data$Occupation, modified_data$Income)
print(Occupation_Income_table)

ggplot(data = modified_data, mapping = aes(x = Occupation, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Occupation vs Income', x = "Occupation",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

```{r}
#| message: false
# Sex
Sex_Income_table <- table(modified_data$Sex, modified_data$Income)
print(Sex_Income_table)

ggplot(data = modified_data, mapping = aes(x = Sex, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Sex vs Income', x = "Sex",
       y = "Count") +
  theme_minimal()
```

```{r}
#| message: false
# Nationality
Nationality_Income_table <- table(modified_data$Nationality, modified_data$Income)
print(Nationality_Income_table)

ggplot(data = modified_data, mapping = aes(x = Nationality, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Nationality vs Income', x = "Nationality",
       y = "Count") +
  theme_minimal()
```

```{r}
#| message: false
# Education level
Education_Income_table <- table(modified_data$Education_level, modified_data$Income)
print(Education_Income_table)

ggplot(data = modified_data, mapping = aes(x = Education_level, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Education level vs Income', x = "Education level",
       y = "Count") +
  theme_minimal()
```

```{r}
#| message: false
# Partner
Partner_Income_table <- table(modified_data$Has_partner, modified_data$Income)
print(Partner_Income_table)

ggplot(data = modified_data, mapping = aes(x = Has_partner, fill = Income)) +
  geom_bar(position = 'dodge') +
  scale_fill_manual(values = c("<=50K" = "#003B5C", ">50K" =
                                 "#ff7f0e")) +
  labs(title = 'Partner vs Income', x = "Partner",
       y = "Count") +
  theme_minimal()
```

```{r}
#| message: false
# Violin plots and boxplots for numerical continuous variables
# Age
ggplot(data = modified_data, aes(x = Income, y = Age, fill = Income)) +
  geom_violin(col = 'transparent') +
  scale_fill_manual(values = c("<=50K" = "#7f7f7f", ">50K" =
                                 "#ffdd57")) +
  labs(title = "Age vs Income", x = "Income", y = "Age") +
  theme_minimal()

ggplot(modified_data, aes(x = Income, y = Age, fill = Income)) +
  geom_boxplot() +
  labs(title = "Age vs Income", x = "Age", y = "Hours per Week") +
  theme_minimal()

# Hours per week
ggplot(data = modified_data, aes(x = Income, y = Hours_PW, fill = Income)) +
  geom_violin(col = 'transparent') +
  scale_fill_manual(values = c("<=50K" = "#7f7f7f", ">50K" =
                                 "#ffdd57")) +
  labs(title = "Hours Per Week vs Income", x = "Income", y = "Hours_PW") +
  theme_minimal()

ggplot(modified_data, aes(x = Income, y = Hours_PW, fill = Income)) +
  geom_boxplot() +
  labs(title = "Hours Per Week vs Income", x = "Income", y = "Hours per Week") +
  theme_minimal()
```

# 3. Data Splitting: Training set and Test set

```{r}
#| message: false
library(caret)
# set random seed
set.seed(123)

# create training index for stratified sampling
training_index <- createDataPartition(modified_data$Income, p = 0.8, list = FALSE)

# split data into training data and test data
training_data <- modified_data[training_index, ]
test_data  <- modified_data[-training_index, ]
str(training_data)
str(test_data)

# Replace Spaces and ampersand with underscores _
training_data$Occupation <- gsub(" & ", "_", training_data$Occupation)  
training_data$Occupation <- gsub(" ", "_", training_data$Occupation)    

test_data$Occupation <- gsub(" & ", "_", test_data$Occupation)  
test_data$Occupation <- gsub(" ", "_", test_data$Occupation)

# Make sure the variable is still a factor type
training_data$Occupation <- as.factor(training_data$Occupation)

test_data$Occupation <- as.factor(test_data$Occupation)

# View the modified categories
levels(training_data$Occupation)

levels(test_data$Occupation)




# check dimension
dim(training_data)
dim(test_data)

# check distribution in both datasets
prop.table(table(training_data$Income))
prop.table(table(test_data$Income))
```

# 4. Model fitting

## 1. GLM

```{r}
#| message: false
library(MASS)
library(caret)
library(pROC)
library(pscl)

# Create dummy variables
dummy_variables <- model.matrix(~ Occupation - 1, data = training_data)
education_dummy <- model.matrix(~ Education_level - 1, data = training_data)
nationality_dummy <- model.matrix(~ Nationality - 1, data = training_data)
has_partner_dummy <- model.matrix(~ Has_partner - 1, data = training_data)
sex_dummy <- model.matrix(~ Sex - 1, data = training_data)
has_partner_dummy <- model.matrix(~ Has_partner - 1, data = training_data)

#Check new variables
colnames(dummy_variables)
colnames(education_dummy)
colnames(nationality_dummy)
colnames(has_partner_dummy)
colnames(sex_dummy)

# Add the dummy variables into the original dataset
training_data <- cbind(training_data, dummy_variables)
training_data <- cbind(training_data, education_dummy)
training_data <- cbind(training_data, nationality_dummy)
training_data <- cbind(training_data, has_partner_dummy)
training_data <- cbind(training_data, sex_dummy)

full_model <- glm(Income ~ (Age + Occupation + Sex + Hours_PW + Nationality + Education_level + Has_partner)^2, 
                  data = training_data, family = binomial)


step_model <- stepAIC(full_model, direction = "both", trace = FALSE)
summary(step_model)


test_data$predicted_prob <- predict(step_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob >= 0.5, ">50K", "<=50K")
test_data$predicted_class <- factor(test_data$predicted_class, levels = levels(test_data$Income))


conf_matrix <- confusionMatrix(test_data$predicted_class, test_data$Income)
print(conf_matrix)


roc_curve <- roc(test_data$Income, test_data$predicted_prob)
auc_score <- auc(roc_curve)
print(auc_score)
plot(roc_curve, col = "blue", main = "ROC Curve for Logistic Regression")


pseudo_r2 <- pR2(step_model)
print(pseudo_r2)
```

```{r}
# Load necessary libraries
library(MASS)  # stepAIC()
library(car)   # VIF
library(pROC)  # AUC calculation
library(dplyr) # Data manipulation

# Step 1: Select the best main effects model
base_model <- glm(Income ~ Age + Occupation + Sex + Hours_PW + Nationality + Education_level + Has_partner, 
                  data = training_data, family = binomial)

step_base_model <- stepAIC(base_model, direction = "both", trace = FALSE)

summary(step_base_model)  # View the optimized main effects model

# Step 2: Check interaction effects
interaction_model <- glm(Income ~ Age + Occupation + Hours_PW + Nationality + 
                           Education_level + Has_partner +
                           Education_level:Occupation + Hours_PW:Occupation,
                         data = training_data, family = binomial)

anova(step_base_model, interaction_model, test = "Chisq")  # Perform likelihood ratio test

# Step 3: Use stepAIC() to select the final model
final_model <- stepAIC(interaction_model, direction = "both", trace = FALSE)

summary(final_model)  # View the final model

# Step 4: Generate dummy variables (Ensure that OccupationService_Labor_jobs exists)
training_data$Occupation <- as.factor(training_data$Occupation)
occupation_dummy <- model.matrix(~ Occupation - 1, data = training_data) %>% as.data.frame()
training_data <- cbind(training_data, occupation_dummy)

# Check if `OccupationService_Labor_jobs` is included
colnames(training_data)

# Step 5: Refit the final optimized model
final_model_refined <- glm(Income ~ Age + OccupationService_Labor_jobs + Hours_PW + 
                             Nationality + Education_level + Has_partner + 
                             OccupationService_Labor_jobs:Hours_PW, 
                           family = binomial, data = training_data)

summary(final_model_refined)  # Check statistical significance

# Step 6: Check VIF (Multicollinearity)
vif(final_model_refined)

# Standardize variables
training_data$Hours_PW <- scale(training_data$Hours_PW)
training_data$OccupationService_Labor_jobs <- scale(training_data$OccupationService_Labor_jobs)

# Refit the model
final_model_refined <- glm(Income ~ Age + OccupationService_Labor_jobs + Hours_PW + 
                             Nationality + Education_level + Has_partner + 
                             OccupationService_Labor_jobs:Hours_PW, 
                           family = binomial, data = training_data)

# Recheck VIF
vif(final_model_refined)

# Step 7: Compute AUC to evaluate classification performance
roc_curve <- roc(training_data$Income, fitted(final_model_refined))
auc(roc_curve)  # Compute AUC value
plot(roc_curve) # Plot ROC curve

```
In the model optimization process, we first selected the optimal main effects model by applying main effect selection, ANOVA interaction testing, and stepwise AIC optimization, aiming to construct a more robust model. Variables with statistical significance (p < 0.05) were retained, while non-significant variables (p > 0.1) were removed. The residual deviance of the current model was 773.01, indicating that it explained a substantial amount of the data variability. The AIC value was 793.01, and the Number of Fisher Scoring iterations was 6, suggesting that the model was relatively stable.

Subsequently, we examined potential interaction effects to assess their additional explanatory power. The ANOVA test revealed significant interaction effects (p < 0.05), leading to further optimization using stepAIC(), which resulted in the final model. To evaluate the model’s predictive performance, we computed the AUC, obtaining a value of 0.8871, which suggests a strong predictive ability.

We then examined multicollinearity using the VIF test. The results initially indicated that OccupationService_Labor_jobs (VIF = 12.626) and OccupationService_Labor_jobs:Hours_PW (VIF = 12.196) had VIF values exceeding 10, suggesting a severe multicollinearity issue. However, since OccupationService_Labor_jobs:Hours_PW was statistically significant (p < 0.05), it was not appropriate to simply remove this interaction term. Instead, we applied variable standardization to mitigate collinearity. After standardizing the relevant variables, a subsequent VIF test confirmed that all VIF values were below 2, indicating that the collinearity issue was effectively resolved.


## 2. Random Forests

```{r}
library(randomForest)  
library(caret)        
library(pROC)         
library(rpart.plot)
          
training_data$Income <- as.factor(training_data$Income)
test_data$Income <- as.factor(test_data$Income)

# Ensure that the target variable is a factor type
training_data$Income <- as.factor(training_data$Income)
test_data$Income <- as.factor(test_data$Income)

# Constructing random forest model
set.seed(123)  
rf_model <- randomForest(
  Income ~ .,                
  data = training_data,      
  ntree = 500,               
  mtry = sqrt(ncol(training_data) - 1), importance = TRUE, proximity = TRUE)  

# View model summary
print(rf_model)
library(rpart.plot)

# Extract the first tree in a random forest
tree <- getTree(rf_model, k = 1, labelVar = TRUE)
print(tree)

# Extract OOB errors
oob_errors <- rf_model$err.rate

# Convert OOB errors to a data frame for plotting
oob_df <- data.frame(
  Trees = 1:nrow(oob_errors),
  OOB_Error = oob_errors[, "OOB"])

# Plot the OOB error curve
ggplot(oob_df, aes(x = Trees, y = OOB_Error)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "OOB Error Curve for Random Forest",
    x = "Number of Trees",
    y = "OOB Error") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12))


# Variable importance analysis
importance(rf_model)
varImpPlot(rf_model, main = "Variable Importance Plot")

# Prediction on test set
test_data$predicted_prob <- predict(rf_model, newdata = test_data, type = "prob")[, 2]
test_data$predicted_class <- predict(rf_model, newdata = test_data, type = "response")

# Calculate confusion matrix
conf_matrix <- confusionMatrix(test_data$predicted_class, test_data$Income)
print(conf_matrix)

# Calculate ROC-AUC
roc_curve <- roc(test_data$Income, test_data$predicted_prob)
auc_score <- auc(roc_curve)
print(auc_score)
plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")
```

The OOB error rate is 17.06%, which means the model misclassifies approximately 17.06% of the training data on average. This is a good estimate of the model's generalization error. The model achieves good accuracy (80.59%) and a high AUC score (0.8632), indicating strong overall performance. However, the lower specificity for the \>50K class highlights the need to address class imbalance and refine the model further. Has_partner is the most important variable for accuracy, suggesting it plays a critical role in predicting income. From the variable importance plot, we can indicate that Age and Education_level are also highly important, which aligns with real-world intuition (income often correlates with age and education).
